{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f57ae94",
   "metadata": {},
   "source": [
    "# ISIC Challenge 2016 - Lesion Segmentation\n",
    "Student: Braian Olmiro Dias\n",
    "\n",
    "## 1. Set-up of the project\n",
    "Based on the data available on the [ISIC Challenge 2016](https://challenge.isic-archive.com/landing/2016/37/), two models were trained for **lesion segmentation** based on the Keras tutorial [here](https://keras.io/examples/vision/oxford_pets_image_segmentation/). \n",
    "\n",
    "The libraries used were:\n",
    "\n",
    "* Tensorflow 2.3.0\n",
    "* Scikit-learn (for metrics)\n",
    "* Matplotlib (for plotting)\n",
    "* Numpy\n",
    "\n",
    "From the 900 images available for training:\n",
    "\n",
    "* 750 were used as **training** data\n",
    "* 150 were used as **validation** data\n",
    "\n",
    "The data split used for training and validation was randomly chosen.\n",
    "\n",
    "In addition, a separate **test set** available at the challenge website was dowloaded, containing 379 images in total. This dataset was used to compare the models trained\n",
    "\n",
    "Since the **segmentation mask** provided was a greyscale image, I tried to load it as such. However, the range of the data was still between 0 and 255. In order to use the architecture proposed in the tutorial I forced the values to be discrete [0,1], 0 representing background and 1 foreground (the lesion).\n",
    "\n",
    "## 2. Experiments\n",
    "\n",
    "### 2.1 Model\n",
    "The model used in the project was adapted from the Keras tutorial mentioned above, a **U-Net Xception-style** architecture. I did not change the architecture, the only notable change was the number of classes that now is 2 - foreground and background.\n",
    "\n",
    "I tested different optimization functions: Adam, RMSProp and SGD with Momentum. The model was trained with `sparse_categorical_crossentropy` loss.\n",
    "\n",
    "### 2.2 Data\n",
    "My baseline model used the data split as described above, and no augmentation or constant shuffling was applied. Later, I added shuffle at the end of each epoch and two different type of augmentation:\n",
    "\n",
    "* **Random Rotation**: with probability of occuring .6 for each image loaded during training\n",
    "* **Random Center Crop**: with the same probability. The crop was defined by a random value between 1 and 20% of the image side (top, left, bottom, right), independently. This way I could crop the 4 sides of the image simulating a Center Crop.\n",
    "\n",
    "### 2.3 Extras\n",
    "For all experiments I added an [EarlyStopping](https://keras.io/api/callbacks/early_stopping/) callback to stop the training after 10 epochs without any improvement bigger than 0.01 on the validation loss.\n",
    "\n",
    "## 3. Results\n",
    "The baseline model performed well, converging after 36 epochs. However, the model with data augmentation did not converge well. Despite trying different optimization functions and debugging the code, I was unable to detect the root cause of the problem. It is shown below the training curves for both models:\n",
    "\n",
    "![fig 1](report/train_curves_baseline.png)![fig 2](report/train_curves_aug.png)\n",
    "\n",
    "After looking into some examples of the validation data it was possible to see that the augmentation model does not produce good results. My intuition is that the custom functions for cropping and rotation are producing undesirable results despite producing decent results when testing separetely. In the future, I could adapt the whole data pipeline to use an augmentation library such as [albumentations](https://albumentations.ai/).\n",
    "\n",
    "Below I report the metrics for both baseline and the model with augmentation on the **test set**. Both in the end were trained using RMSProp, however none of the optimizers tested improved the augmentation model.\n",
    "\n",
    "![fig 3](report/metrics.png)\n",
    "![fig 4](report/roc_baseline.png)\n",
    "\n",
    "Looking at some prediction examples of the best model (baseline) it is possible to see the model is learning the shapes of the lesions, however the test results show the model is not **generalizing** well, since the images from the validation set look more accurate. Below is an example of the baseline model on the test set.\n",
    "\n",
    "![fig 5](report/sample_baseline.png)\n",
    "\n",
    "## 4. Conclusion\n",
    "Unfortunately due to limited amount of time the model with augmentation did not produce decent results (see image below). I believe with the addition of augmentation the model would generalize better and achieve better results.\n",
    "Further investigation is needed in order to fix my custom augmentation, but in the future I'll try to use an augmentation library to avoid such problems.\n",
    "\n",
    "For the complete code and report please refer to the Annex I - Baseline model and Annex II - Augmentation. The whole source code can be seen here https://github.com/bodias/ISIC_Challenge2016\n",
    "\n",
    "![fig 6](report/sample_augmentation.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c4f76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
